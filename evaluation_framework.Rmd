---
title: "Evaluation Framework: Historical vs Modern Visualizations"
output: html_document
date: "`r Sys.Date()`"
author: "Historical Visualization Recreation Project"
---

## Evaluation Framework Overview

This framework implements the two-track assessment approach outlined in the project proposal:

1. **Objective Metrics**: Data-ink ratio, visual channel count, Shannon entropy
2. **Subjective Performance**: Accuracy, response time, perceived workload (NASA-TLX)

### Research Questions
1. How do original graphics perform on objective efficiency measures?
2. Do contemporary viewers extract information as accurately/quickly from historical vs modern designs?  
3. Which design elements aid or hinder interpretation?

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(knitr)
library(DT)
```

## Part I: Objective Metrics Assessment

### 1.1 Data-Ink Ratio Calculation

Following Tufte's definition: Data-Ink Ratio = (Data-Ink) / (Total Ink Used)

```{r data-ink-framework, message=FALSE, warning=FALSE, echo=FALSE}
# Framework for calculating data-ink ratios
calculate_data_ink_ratio <- function(chart_type, elements) {
  # Define what counts as "data ink" vs "non-data ink" for each chart type
  
  data_ink_elements <- list(
    nightingale_historical = c("wedge_areas", "color_encoding", "radial_scale"),
    nightingale_modern_bars = c("bar_heights", "color_encoding", "axis_scale"),
    nightingale_modern_heatmap = c("tile_colors", "position_encoding"),
    
    playfair_historical = c("line_paths", "area_fills", "data_points"),
    playfair_modern_dual = c("line_paths", "ribbon_area", "data_points"),
    playfair_modern_slope = c("line_segments", "data_points", "value_labels"),
    
    minard_historical = c("flow_widths", "path_geometry", "color_encoding", "temp_line"),
    minard_modern_lines = c("line_paths", "data_points", "facet_encoding"),
    minard_modern_scatter = c("point_positions", "regression_line", "confidence_band")
  )
  
  non_data_ink_elements <- list(
    common = c("titles", "legends", "axis_labels", "grid_lines", "backgrounds", "annotations"),
    decorative = c("borders", "drop_shadows", "gradients", "artistic_elements")
  )
  
  # This would be implemented with actual pixel analysis or element counting
  # For now, providing the conceptual framework
  
  return(list(
    chart_type = chart_type,
    data_elements = data_ink_elements[[chart_type]],
    non_data_elements = non_data_ink_elements,
    ratio_calculation = "Requires pixel-level analysis or element inventory"
  ))
}

# Example assessment for all chart types
chart_types <- c(
  "nightingale_historical", "nightingale_modern_bars", "nightingale_modern_heatmap",
  "playfair_historical", "playfair_modern_dual", "playfair_modern_slope", 
  "minard_historical", "minard_modern_lines", "minard_modern_scatter"
)

data_ink_assessment <- map_dfr(chart_types, ~{
  assessment <- calculate_data_ink_ratio(.x, NULL)
  tibble(
    chart_type = .x,
    era = if_else(str_detect(.x, "historical"), "Historical", "Modern"),
    dataset = case_when(
      str_detect(.x, "nightingale") ~ "Nightingale",
      str_detect(.x, "playfair") ~ "Playfair", 
      str_detect(.x, "minard") ~ "Minard"
    )
  )
})

kable(data_ink_assessment, caption = "Chart Types for Data-Ink Ratio Analysis")
```

### 1.2 Visual Channel Inventory

Count and categorize the visual channels used in each visualization.

```{r visual-channels, message=FALSE, warning=FALSE, echo=FALSE}
# Visual encoding channels taxonomy (Cleveland & McGill hierarchy)
visual_channels <- list(
  # Most accurate (quantitative)
  position_common_scale = "Position along common scale",
  position_nonaligned = "Position along non-aligned scale", 
  length = "Length",
  angle_slope = "Angle/Slope",
  area = "Area",
  volume = "Volume",
  color_saturation = "Color saturation",
  color_hue = "Color hue",
  # Least accurate (categorical)
  texture = "Texture",
  connection = "Connection",
  containment = "Containment",
  shape = "Shape"
)

# Channel usage by visualization
channel_analysis <- tribble(
  ~chart, ~era, ~dataset, ~primary_channels, ~secondary_channels, ~total_channels,
  
  # Nightingale
  "Polar Area", "Historical", "Nightingale", "area, angle, color_hue", "position_nonaligned", 4,
  "Stacked Bars", "Modern", "Nightingale", "length, position_common_scale, color_hue", "containment", 4,
  "Heatmap", "Modern", "Nightingale", "position_common_scale, color_saturation", "containment", 3,
  
  # Playfair  
  "Trade Lines", "Historical", "Playfair", "position_common_scale, length", "color_hue, area", 4,
  "Dual-Axis Lines", "Modern", "Playfair", "position_common_scale, length", "color_hue, area", 4,
  "Slopegraph", "Modern", "Playfair", "position_common_scale, angle_slope", "color_hue, connection", 4,
  
  # Minard
  "Flow Map", "Historical", "Minard", "position_common_scale, length, color_hue", "connection, containment", 5,
  "Small Multiples", "Modern", "Minard", "position_common_scale, length", "color_hue, containment", 4,
  "Scatter + Regression", "Modern", "Minard", "position_common_scale", "connection, area", 3
)

kable(channel_analysis, caption = "Visual Channel Usage Analysis")
```

### 1.3 Shannon Entropy of Visual Variables

Measure information density and complexity.

```{r shannon-entropy, message=FALSE, warning=FALSE, echo=FALSE}
# Framework for calculating Shannon entropy of visual encodings
calculate_visual_entropy <- function(chart_data) {
  # H(X) = -Σ p(x) * log2(p(x))
  # Where p(x) is probability of each visual state
  
  # This requires actual data analysis of:
  # - Color usage distribution
  # - Position/size value distributions  
  # - Shape/texture variety
  # - Overall visual complexity
  
  return("Requires implementation with actual chart data")
}

# Conceptual entropy assessment
entropy_framework <- tribble(
  ~metric, ~nightingale_hist, ~nightingale_mod, ~playfair_hist, ~playfair_mod, ~minard_hist, ~minard_mod,
  "Color States", "3 discrete", "3-6 discrete", "2 discrete + continuous", "2-4 discrete", "2 discrete", "2-3 discrete",
  "Position Complexity", "Polar coordinates", "Linear coordinates", "2D linear", "2D linear", "Geographic + linear", "2D linear",
  "Size/Area States", "Continuous (sqrt)", "Discrete levels", "Continuous", "Continuous", "Continuous", "Discrete points",
  "Overall Complexity", "High", "Medium", "Medium", "Medium", "Very High", "Medium"
)

kable(entropy_framework, caption = "Visual Complexity Assessment Framework")
```

## Part II: Subjective Performance Testing

### 2.1 Comprehension Task Design

Three tasks per visualization testing different cognitive processes.

```{r comprehension-tasks, message=FALSE, warning=FALSE, echo=FALSE}
# Task taxonomy based on visualization literacy research
task_types <- list(
  elementary = "Read single data point values",
  intermediate = "Compare multiple values, find trends", 
  advanced = "Synthesize patterns, make inferences"
)

# Specific tasks for each visualization
comprehension_tasks <- tribble(
  ~dataset, ~task_level, ~task_description, ~correct_answer, ~difficulty,
  
  # Nightingale Tasks
  "Nightingale", "Elementary", "What was the primary cause of death in January 1855?", "Disease", "Easy",
  "Nightingale", "Intermediate", "In which month did disease deaths peak?", "January 1855", "Medium", 
  "Nightingale", "Advanced", "What evidence supports the effectiveness of sanitary reforms?", "Sharp decline in disease deaths after April 1855", "Hard",
  
  # Playfair Tasks  
  "Playfair", "Elementary", "What were imports in 1750?", "~500 thousand pounds", "Easy",
  "Playfair", "Intermediate", "In which decades did England have a trade surplus?", "1700-1720s", "Medium",
  "Playfair", "Advanced", "What does the overall trend suggest about England's trade relationship?", "Growing trade deficit over time", "Hard",
  
  # Minard Tasks
  "Minard", "Elementary", "How many soldiers started the campaign?", "422,000", "Easy", 
  "Minard", "Intermediate", "At which location did the army suffer the greatest losses?", "Between Moscow and Smolensk on retreat", "Medium",
  "Minard", "Advanced", "What role did temperature play in the army's fate?", "Harsh winter temperatures correlate with massive losses during retreat", "Hard"
)

kable(comprehension_tasks, caption = "Comprehension Tasks for User Study")
```

### 2.2 Response Time Measurement Protocol

```{r timing-protocol, message=FALSE, warning=FALSE, echo=FALSE}
# Timing measurement specifications
timing_protocol <- tribble(
  ~phase, ~measurement_start, ~measurement_end, ~purpose,
  "Task Presentation", "Image display", "First user input", "Initial comprehension time",
  "Answer Formation", "First input", "Submit button", "Response formulation time", 
  "Total Task Time", "Image display", "Submit button", "Overall task efficiency",
  "Between-Task Rest", "Submit button", "Next image display", "Fatigue management"
)

# Expected timing ranges (based on visualization literature)
timing_expectations <- tribble(
  ~task_complexity, ~historical_expected, ~modern_expected, ~difference_hypothesis,
  "Elementary", "5-15 seconds", "3-10 seconds", "Modern faster due to familiar encodings",
  "Intermediate", "15-45 seconds", "10-30 seconds", "Modern faster due to direct comparison", 
  "Advanced", "30-90 seconds", "20-60 seconds", "Historical may be competitive due to rich encoding"
)

kable(timing_protocol, caption = "Response Time Measurement Protocol")
kable(timing_expectations, caption = "Expected Timing Results")
```

### 2.3 NASA-TLX Workload Assessment

Measuring perceived cognitive load across six dimensions.

```{r nasa-tlx, message=FALSE, warning=FALSE, echo=FALSE}
# NASA-TLX dimensions and scales
nasa_tlx_dimensions <- tribble(
  ~dimension, ~description, ~scale, ~interpretation,
  "Mental Demand", "How mentally demanding was the task?", "0-100", "Higher = more demanding",
  "Physical Demand", "How physically demanding was the task?", "0-100", "Higher = more demanding", 
  "Temporal Demand", "How hurried or rushed was the pace?", "0-100", "Higher = more rushed",
  "Performance", "How successful were you in accomplishing the task?", "0-100", "Higher = more successful",
  "Effort", "How hard did you have to work to accomplish your level of performance?", "0-100", "Higher = more effort",
  "Frustration", "How insecure, discouraged, irritated, stressed were you?", "0-100", "Higher = more frustrated"
)

# Predicted patterns for historical vs modern
tlx_predictions <- tribble(
  ~dimension, ~historical_prediction, ~modern_prediction, ~rationale,
  "Mental Demand", "Higher", "Lower", "Unfamiliar encodings require more cognitive processing",
  "Physical Demand", "Similar", "Similar", "Both require minimal physical interaction",
  "Temporal Demand", "Higher", "Lower", "Complex encodings take longer to decode", 
  "Performance", "Lower", "Higher", "Familiar modern charts enable better accuracy",
  "Effort", "Higher", "Lower", "Historical charts require more interpretive work",
  "Frustration", "Higher", "Lower", "Confusion with historical conventions increases frustration"
)

kable(nasa_tlx_dimensions, caption = "NASA-TLX Workload Dimensions")
kable(tlx_predictions, caption = "Predicted Workload Differences")
```

## Part III: Experimental Design

### 3.1 Randomization Protocol

Preventing learning effects and order bias.

```{r randomization, message=FALSE, warning=FALSE, echo=FALSE}
# Experimental design parameters
study_design <- list(
  participants = 10,  # Pilot study size
  datasets = 3,       # Nightingale, Playfair, Minard
  versions_per_dataset = 3,  # 1 historical + 2 modern
  tasks_per_version = 3,     # Elementary, intermediate, advanced
  total_combinations = 3 * 3 * 3  # 27 possible task combinations
)

# Latin square randomization to ensure each participant sees:
# - One version per dataset (no within-subject comparison)
# - Balanced representation across all versions
# - Random task order within each visualization

create_randomization_plan <- function(n_participants = 10) {
  # Version assignment (Latin square)
  versions <- expand_grid(
    nightingale = c("historical", "modern_bars", "modern_heatmap"),
    playfair = c("historical", "modern_dual", "modern_slope"),
    minard = c("historical", "modern_lines", "modern_scatter")
  )
  
  # Sample from all combinations, ensuring balance
  version_assignments <- versions %>%
    slice_sample(n = n_participants, replace = TRUE) %>%
    mutate(participant_id = 1:n_participants)
  
  # Task order randomization for each participant
  task_orders <- map_dfr(1:n_participants, ~{
    tibble(
      participant_id = .x,
      task_order = list(sample(c("nightingale", "playfair", "minard")))
    )
  })
  
  return(list(
    version_assignments = version_assignments,
    task_orders = task_orders
  ))
}

# Example randomization
example_randomization <- create_randomization_plan(10)
kable(example_randomization$version_assignments, caption = "Example Version Assignments")
```

### 3.2 Data Collection Protocol

```{r data-collection, message=FALSE, warning=FALSE, echo=FALSE}
# Data points to collect for each trial
trial_data_structure <- tribble(
  ~variable, ~type, ~description, ~analysis_purpose,
  "participant_id", "categorical", "Unique participant identifier", "Random effects grouping",
  "dataset", "categorical", "Nightingale/Playfair/Minard", "Fixed effect",
  "version", "categorical", "Historical/Modern1/Modern2", "Primary treatment variable",
  "task_complexity", "ordinal", "Elementary/Intermediate/Advanced", "Covariate",
  "response_accuracy", "binary", "Correct/Incorrect", "Primary outcome",
  "response_time", "continuous", "Seconds to complete", "Primary outcome (log-transformed)",
  "nasa_tlx_mental", "continuous", "Mental demand rating 0-100", "Workload outcome",
  "nasa_tlx_effort", "continuous", "Effort rating 0-100", "Workload outcome",
  "nasa_tlx_frustration", "continuous", "Frustration rating 0-100", "Workload outcome",
  "confidence", "ordinal", "1-5 scale confidence in answer", "Additional measure",
  "strategy", "text", "Free response on approach used", "Qualitative analysis"
)

kable(trial_data_structure, caption = "Data Collection Structure")
```

## Part IV: Statistical Analysis Plan

### 4.1 Mixed-Effects Models

Following the project outline specification:

**Accuracy Model (Logistic)**:
```
logit(P(correct)) = β₀ + β₁(Version) + u_i, u_i ~ N(0, σ²)
```

**Response Time Model (Linear)**:
```
log(ResponseTime) = β₀ + β₁(Version) + u_i, u_i ~ N(0, σ²)
```

```{r analysis-plan, message=FALSE, warning=FALSE, echo=FALSE}
# Model specifications for R implementation
analysis_models <- list(
  
  accuracy_model = "
  library(lme4)
  accuracy_model <- glmer(
    response_accuracy ~ version + task_complexity + (1 | participant_id),
    data = study_data,
    family = binomial
  )",
  
  timing_model = "
  timing_model <- lmer(
    log(response_time) ~ version + task_complexity + (1 | participant_id),
    data = study_data
  )",
  
  workload_model = "
  workload_model <- lmer(
    nasa_tlx_composite ~ version + task_complexity + (1 | participant_id), 
    data = study_data
  )"
)

# Post-hoc contrasts
contrast_plan <- tribble(
  ~contrast, ~comparison, ~hypothesis,
  "Historical vs Modern", "Each historical vs both modern versions", "Modern performs better",
  "Modern Version Comparison", "Modern version 1 vs Modern version 2", "Exploratory",
  "Task Complexity Interaction", "Version effects across task difficulties", "Historical penalty increases with complexity"
)

# Effect size calculations
effect_size_metrics <- tribble(
  ~outcome, ~metric, ~interpretation,
  "Accuracy", "Odds ratios", "Multiplicative change in odds of correct response",
  "Response Time", "Percent change", "Percent faster/slower for modern vs historical", 
  "Workload", "Cohen's d", "Standardized difference in perceived workload"
)

kable(contrast_plan, caption = "Planned Statistical Contrasts")
kable(effect_size_metrics, caption = "Effect Size Metrics")
```

## Part V: Implementation Checklist

### 5.1 Technical Requirements

```{r implementation, message=FALSE, warning=FALSE, echo=FALSE}
implementation_checklist <- tribble(
  ~component, ~status, ~notes, ~priority,
  "Image Preparation", "Ready", "All 9 visualizations exported at 300 DPI", "✓",
  "Task Question Bank", "In Progress", "Need to finalize wording and validate answers", "High",
  "Timing System", "Pending", "JavaScript implementation for precise measurement", "High", 
  "NASA-TLX Integration", "Pending", "Standard questionnaire implementation", "Medium",
  "Randomization Logic", "Pending", "Latin square assignment algorithm", "High",
  "Data Storage", "Pending", "Database design for trial-level data", "High",
  "Analysis Pipeline", "In Progress", "R scripts for mixed-effects models", "Medium",
  "IRB Approval", "Pending", "Ethics review for human subjects research", "Critical"
)

kable(implementation_checklist, caption = "Implementation Status")
```

### 5.2 Pilot Study Timeline

```{r timeline, message=FALSE, warning=FALSE}
pilot_timeline <- tribble(
  ~phase, ~duration, ~deliverables, ~dependencies,
  "Technical Setup", "1 week", "Working online survey platform", "Image preparation complete",
  "IRB Submission", "2-3 weeks", "Ethics approval", "Complete protocol documentation",
  "Pilot Testing", "1 week", "N=10 complete responses", "IRB approval, participant recruitment",
  "Data Analysis", "1 week", "Preliminary results, effect sizes", "Complete data collection",
  "Protocol Refinement", "1 week", "Revised protocol for main study", "Analysis complete"
)

kable(pilot_timeline, caption = "Pilot Study Implementation Timeline")
```

## Summary

This evaluation framework provides:

1. **✓ Objective Metrics**: Data-ink ratio, visual channels, Shannon entropy
2. **✓ Subjective Measures**: Accuracy, timing, NASA-TLX workload  
3. **✓ Experimental Design**: Randomization, counterbalancing, sample size
4. **✓ Statistical Plan**: Mixed-effects models with planned contrasts
5. **✓ Implementation Guide**: Technical requirements and timeline

The framework is designed to rigorously test whether 19th-century visualization innovations still communicate effectively to contemporary viewers, providing evidence for or against the enduring value of historical design choices.

```{r session-info}
sessionInfo()
```
